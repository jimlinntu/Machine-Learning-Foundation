1. N > 45 時, 選 100
2. 選 H^1126 = H

N = 6
d = 4
設定 X = np.array([[1,0,0,0], [0,1,0,0], [0, 0, 1, 0], [0, 0, 0, 1], [1, 0, 0, 0], [1, 0, 0, 0]])

實際上有 4 個 1 (N - (d+1) = 6 - (4+1) = 1)
然後沒有值是 bigger than 1
array([1.00000000e+00, 1.84889275e-32, 4.53246652e-17, 1.00000000e+00,
       1.00000000e+00, 1.00000000e+00]), array([[-5.77350269e-01, -2.10641604e-16, -6.66666667e-01,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  1.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  1.00000000e+00],
       [-5.77350269e-01,  7.07106781e-01,  7.41581624e-01,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [-5.77350269e-01, -7.07106781e-01, -7.49149571e-02,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]))

3. 選 max(0, 1 - ywx)
4. 選 (1/2) * exp(-ywTx)
5. 選 max(0, -yw^Tx)
因為取完 gradient 是
0(yw^Tx >= 0) 或 yx(-yw^Tx > 0)
6. (-2, 0)
7. 2.825
8. (1.5, 4, -1, -2, 0, 3)
9. -H^(-1) gradient
10. 2.361
11. 選 x1,x2,x3,x4,x5,x6  quadratic 就包含 linear 和 constant
答案是: x1, x2, x3, x4, x5, x6（但這網頁的解法是錯的）
https://blog.csdn.net/a1015553840/article/details/51103645
quadratic hypothesis 可以造出各種 2 次曲線：雙曲線、拋物線、圓形、橢圓形
考慮：
6 個 + 時：可用大圓形包起
5 個 + 時：可用圓形包起讓任何一個點為 -
4 個 + 時：把所有的 - 枚舉出來（x1x2, x1x3.....） 即可發現可以把這些點變成 - 的
3 個 + 時：也是把所有 - 枚舉出來（20種），比較麻煩的：x1x2x5 可用雙曲線把裡面變成負的
2 個 + 時：直接把 4 個 + 的 weight 加上負號就可以了
1 個 + 時：同理
0 個 + 時：同理


在圖上畫出, 並考慮各種圓錐曲線在 w^T \phi(x) = 0 的曲線在哪就可以知道
(x1, x2, x3, x4)
12. 這個 transform 的意思就是問說: x 是不是第 n 筆資料
因此轉換過後，z_n 會彼此 orthogonal(事實上就是一個 one hot vector)
always linear separable: 因為 h(z) =  w1 x z1 + w2 x z2 ... + wN x zN, 每次帶進去的時候
h(z_n) = wn 所以一定可以調好 w 讓所有 z_n 都是對的預測
E_in = 0: 因為 PLA 一定會停，會全對
E_out = 0.7: 因為 h(z from testing set) = 0(因為z1 ~zN 一定都是0) -> sign(0) = 1 會錯 70% 的資料

選 E_in = 0.3 錯

13. 0.5
14. 選 x1^2, x1^2 係數為 1.5 的（因為其他項不穩定所以浮動很大）
15. 用 14 題結果來測出來平均是 0.14
用自己的 linear regression 得到的 w_lin 結果是 0.126

16. 選 (1/N)\sum_n^N(ln(\sum_{k=1}^K exp(w_k x_n)) - w_y_n x_n)
17. 選 (1/N)\sum_n=1^N((h_i(x_n) - [y_n == i]) x_n)
18. 0.475
19. 0.220
20. 0.475
